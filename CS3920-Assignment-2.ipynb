{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3920 Assignment 2\n",
    "\n",
    "1. Install the needed libraries into the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp311-cp311-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kurisu\\projects\\cs3920-cw2\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kurisu\\projects\\cs3920-cw2\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kurisu\\projects\\cs3920-cw2\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/11.1 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.0-cp311-cp311-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.0 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.4/8.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.0 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.0-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.0/12.9 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.0/12.9 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.9 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 7.8 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached kiwisolver-1.4.7-cp311-cp311-win_amd64.whl (56 kB)\n",
      "Using cached pillow-11.0.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, contourpy, scikit-learn, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.10.0 numpy-2.2.0 pillow-11.0.0 pyparsing-3.2.0 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the data set into Python using, e.g., load_wine or genfromtxt, as appropriate. In the case of the USPS dataset, merge the original training and test sets into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "\n",
    "uspsZip = {}\n",
    "wine = load_wine()\n",
    "\n",
    "# Load data from both files\n",
    "test_data = np.genfromtxt(\"zip.test\", delimiter=\" \", usecols=np.arange(1, 257))\n",
    "train_data = np.genfromtxt(\"zip.train\", delimiter=\" \", usecols=np.arange(1, 257))\n",
    "\n",
    "# Load targets from both files\n",
    "test_target = np.genfromtxt(\"zip.test\", delimiter=\" \", usecols=0, dtype='int')\n",
    "train_target = np.genfromtxt(\"zip.train\", delimiter=\" \", usecols=0, dtype='float').astype(int)\n",
    "\n",
    "# Combine the two files\n",
    "uspsZip['data'] = np.vstack((test_data, train_data))\n",
    "uspsZip['target'] = np.concatenate((test_target, train_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Divide the dataset into a training set and a test set. You may use the\n",
    "function train_test_split. Use your birthday in the format DDMM as\n",
    "random_state (omit leading zeros if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(wine.data, wine.target, random_state=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_zip_train, X_zip_test, y_zip_train, y_zip_test = train_test_split(uspsZip[\"data\"], uspsZip[\"target\"], random_state=79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using cross-validation and the training set only, estimate the generaliza-\n",
    "tion accuracy of the SVM with the default values of the parameters. You\n",
    "may use the function cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set for ZIP Codes: 0.9708874181720943\n",
      "Accuracy on training set for Wine dataset: 0.6541310541310541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "zip_score = np.mean(cross_val_score(svc, X_zip_train, y_zip_train))\n",
    "wine_score = np.mean(cross_val_score(svc, X_wine_train, y_wine_train))\n",
    "\n",
    "print(f\"Accuracy on training set for ZIP Codes: {zip_score}\")\n",
    "print(f\"Accuracy on training set for Wine dataset: {wine_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the test error rate of the SVM with the default values of parameters,\n",
    "compare it with the estimate obtained in the previous task (task 3), and\n",
    "write your observations in a markdown cell of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error-rate for ZIP Code dataset: 2.8817204301075208%\n",
      "Error-rate for Wine dataset: 31.111111111111114%\n",
      "Accuracy for ZIP Code dataset: 97.11827956989248%\n",
      "Accuracy for Wine dataset: 68.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_zip_train, y_zip_train)\n",
    "zip_acc = svc.score(X_zip_test, y_zip_test) * 100\n",
    "\n",
    "svc.fit(X_wine_train, y_wine_train)\n",
    "wine_acc = svc.score(X_wine_test, y_wine_test) * 100\n",
    "\n",
    "print(f\"Error-rate for ZIP Code dataset: {100 - zip_acc}%\")\n",
    "print(f\"Error-rate for Wine dataset: {100 - wine_acc}%\")\n",
    "\n",
    "print(f\"Accuracy for ZIP Code dataset: {zip_acc}%\")\n",
    "print(f\"Accuracy for Wine dataset: {wine_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a pipeline for SVM involving data normalization and SVC, and\n",
    "use grid search and cross-validation to tune parameters C and gamma for\n",
    "the pipeline, avoiding data snooping and data leakage. You may use\n",
    "the scikit-learn class GridSearchCV (along with other scikit-learn\n",
    "classes). Experiment with different ways of doing normalization (such\n",
    "as StandardScaler, MinMaxScaler, RobustScaler, and Normalizer).\n",
    "Which ways are appropriate for either dataset? (The answer, which should\n",
    "be written in your Jupyter notebook, may depend on the results that you\n",
    "obtain for the next task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "normalisers = [Normalizer(), MinMaxScaler(), RobustScaler(), StandardScaler()]\n",
    "grid_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "def normalise(meth, gridVals, X_test, y_test, X_train, y_train):\n",
    "    pipeline = make_pipeline(meth, SVC())\n",
    "    pipe_param = {\"svc__C\": gridVals, \"svc__gamma\": gridVals}\n",
    "    g_search = GridSearchCV(pipeline, param_grid=pipe_param, cv=len(gridVals), n_jobs=-1)\n",
    "    g_search.fit(X_train, y_train)\n",
    "\n",
    "    return (g_search.score(X_test, y_test), g_search.best_score_, g_search.best_params_), g_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9555555555555556, np.float64(0.8880341880341881), {'svc__C': 100, 'svc__gamma': 100}), (1.0, np.float64(0.9772079772079773), {'svc__C': 0.1, 'svc__gamma': 1}), (1.0, np.float64(0.9703703703703702), {'svc__C': 0.1, 'svc__gamma': 0.1}), (1.0, np.float64(0.9772079772079773), {'svc__C': 1, 'svc__gamma': 0.01})]\n",
      "[GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('normalizer', Normalizer()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]}), GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]}), GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('robustscaler', RobustScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]}), GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})]\n"
     ]
    }
   ],
   "source": [
    "wine_grids = []\n",
    "wine_saved_norm = []\n",
    "\n",
    "for i in normalisers:\n",
    "    grid, norm = normalise(i, grid_values, X_wine_test, y_wine_test, X_wine_train, y_wine_train)\n",
    "    wine_grids.append(grid)\n",
    "    wine_saved_norm.append(norm)\n",
    "\n",
    "print(wine_grids)\n",
    "print(wine_saved_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9750537634408603, np.float64(0.9728947923255324), {'svc__C': 10, 'svc__gamma': 1}), (0.9720430107526882, np.float64(0.9698837310953754), {'svc__C': 10, 'svc__gamma': 0.01}), (0.7359139784946237, np.float64(0.7953542833341047), {'svc__C': 10, 'svc__gamma': 0.01}), (0.9333333333333333, np.float64(0.9327402127911222), {'svc__C': 10, 'svc__gamma': 0.01})]\n",
      "[GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('normalizer', Normalizer()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]}), GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]}), GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('robustscaler', RobustScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]}), GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})]\n"
     ]
    }
   ],
   "source": [
    "zip_grids = []\n",
    "zip_saved_norm = []\n",
    "\n",
    "for i in normalisers:\n",
    "    grid, norm = normalise(i, grid_values, X_zip_test, y_zip_test, X_zip_train, y_zip_train)\n",
    "    zip_grids.append(grid)\n",
    "    zip_saved_norm.append(norm)\n",
    "\n",
    "print(zip_grids)\n",
    "print(zip_saved_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Fit the GridSearchCV object of task 5 to the training set and use it to\n",
    "predict the test labels. Write the resulting test error rate in your Jupyter\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine_acc = []\n",
    "\n",
    "for w_norm in wine_saved_norm:\n",
    "    w_norm: GridSearchCV\n",
    "    w_norm.fit(X_wine_train, y_wine_train)\n",
    "    w_predict = w_norm.predict(X_wine_test)\n",
    "    wine_acc.append((w_norm.estimator.steps[0][0], 100 - (accuracy_score(y_wine_test, w_predict) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_acc = []\n",
    "\n",
    "for z_norm in zip_saved_norm:\n",
    "    z_norm: GridSearchCV\n",
    "    z_norm.fit(X_zip_train, y_zip_train)\n",
    "    z_predict = z_norm.predict(X_zip_test)\n",
    "    zip_acc.append((z_norm.estimator.steps[0][0], 100 - (accuracy_score(y_zip_test, z_predict) * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Implement a cross-conformal predictor. You may use the KFold class for\n",
    "splitting into folds (start from 5 or 10 folds). For computing the conformity\n",
    "scores for each fold, you may use one of the GridSearchCV objects that\n",
    "you created in task 5 in combination with the decision_function method\n",
    "(see Section 3 of Lab Worksheet 9 for examples). Run your cross-conformal\n",
    "predictor on the two datasets, training it on the training set and testing\n",
    "on the test set.\n",
    " - To check its validity, produce a calibration curve, plotting the per-\n",
    "centage of errors made on the test set1 vs the significance level\n",
    "ϵ ∈ [0, 1].\n",
    " - Compute the average false p-value on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_false(y_test, p_values):\n",
    "    in_range = []\n",
    "    for i in range(0, len(y_test)):\n",
    "        for j in np.unique(y_test):\n",
    "            if y_test[i] != j:\n",
    "                in_range.append(p_values[i][j])\n",
    "    \n",
    "    return np.mean(in_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def conform_p_value(grids: list[GridSearchCV], folds, X_train: np.ndarray, y_train: np.array, X_test: np.ndarray, y_test: np.array) -> np.float64:\n",
    "    conform_score = []\n",
    "    p_false_val = []\n",
    "    for gi in range(0, len(grids)):\n",
    "        n_grid = grids[gi]\n",
    "        p_ranks = np.zeros((X_test.shape[0], folds))\n",
    "        p_values = np.zeros_like(p_ranks)\n",
    "        print(f\"Using average false P-Value for: {n_grid}\")\n",
    "        for i, j in KFold(shuffle=True, random_state=0, n_splits=folds).split(X_train):\n",
    "            X_ext = X_train[i]\n",
    "            y_ext = y_train[i]\n",
    "            X_fold = X_train[j]\n",
    "            y_fold = y_train[j]\n",
    "\n",
    "            n_grid.fit(X_ext, y_ext)\n",
    "\n",
    "            fold = n_grid.decision_function(X_fold)\n",
    "            test = n_grid.decision_function(X_ext)\n",
    "            alpha = np.zeros(X_fold.shape[0])\n",
    "            \n",
    "            for k_fold in range(0, X_fold.shape[0], 1):\n",
    "                alpha[k_fold] = fold[k_fold, y_fold[k_fold]]\n",
    "\n",
    "            for k_fold in range(0, X_test.shape[0], 1):\n",
    "                for f in range(0, folds, 1):\n",
    "                    p_ranks[k_fold, f] = p_ranks[k_fold, f] + np.sum(alpha <= test[k_fold, f])\n",
    "            \n",
    "            p_values = (p_ranks + 1) / (X_train.shape[0] + 1)\n",
    "            conform_score.append(p_values)\n",
    "            p_false_val.append([(calculate_avg_false(y_test, p_values))])\n",
    "    return conform_score, p_false_val\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using average false P-Value for: GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('normalizer', Normalizer()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})\n",
      "Using average false P-Value for: GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})\n",
      "Using average false P-Value for: GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('robustscaler', RobustScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})\n",
      "Using average false P-Value for: GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})\n",
      "Wine average false P-Values using;\n",
      "with Normalizer(): [np.float64(0.0632669983416252)]\n",
      "with MinMaxScaler(): [np.float64(0.1323383084577114)]\n",
      "with RobustScaler(): [np.float64(0.16932006633499172)]\n",
      "with StandardScaler(): [np.float64(0.07114427860696518)]\n"
     ]
    }
   ],
   "source": [
    "w_p_vals, w_p_false_vals = conform_p_value(wine_saved_norm, 3,X_wine_train, y_wine_train, X_wine_test, y_wine_test)\n",
    "\n",
    "print(f\"Wine average false P-Values using;\")\n",
    "for i in range(len(normalisers)):\n",
    "    print(f\"with {normalisers[i]}: {w_p_false_vals[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using average false P-Value for: GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('normalizer', Normalizer()),\n",
      "                                       ('svc', SVC())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
      "                         'svc__gamma': [0.01, 0.1, 1, 10, 100]})\n"
     ]
    }
   ],
   "source": [
    "z_p_vals, z_p_false_vals = conform_p_value(zip_saved_norm, 3,X_zip_train, y_zip_train, X_zip_test, y_zip_test)\n",
    "\n",
    "print(f\"Zip code average false P-Values using;\")\n",
    "for i in range(len(normalisers)):\n",
    "    print(f\"with {normalisers[i]}: {z_p_false_vals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. An alternative to implementing a cross-conformal predictor is to experiment with a neural network. Perform tasks 3–6 for the scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score average:\n",
      "Wine Dataset => 0.572934472934473\n",
      "Zip Code Dataset => 0.9609912425499967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_c = MLPClassifier(max_iter=6000)\n",
    "print(f\"Cross-validation score average:\")\n",
    "print(f\"Wine Dataset => {np.mean(cross_val_score(mlp_c, X_wine_train, y_wine_train))}\")\n",
    "print(f\"Zip Code Dataset => {np.mean(cross_val_score(mlp_c, X_zip_train, y_zip_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP on;\n",
      "Wine => 0.13333333333333333\n",
      "Zip Code => 0.9660215053763441\n",
      "\n",
      "Error rate on MLP;\n",
      "Wine => 86.66666666666667%\n",
      "Zip Code => 3.3978494623655875%\n"
     ]
    }
   ],
   "source": [
    "mlp_c.fit(X_wine_train, y_wine_train)\n",
    "w_acc_mlp = mlp_c.score(X_wine_test, y_wine_test)\n",
    "\n",
    "mlp_c.fit(X_zip_train, y_zip_train)\n",
    "z_acc_mlp = mlp_c.score(X_zip_test, y_zip_test)\n",
    "\n",
    "print(\"Accuracy of MLP on;\")\n",
    "print(f\"Wine => {w_acc_mlp}\")\n",
    "print(f\"Zip Code => {z_acc_mlp}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Error rate on MLP;\")\n",
    "print(f\"Wine => {(1 - w_acc_mlp) * 100}%\")\n",
    "print(f\"Zip Code => {(1 - z_acc_mlp) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_m(mlp, normalisers, X_test, y_test, X_train, y_train):\n",
    "    pipeline_param = {\"mlpclassifier__activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "                      \"mlpclassifier__solver\": [\"lbfgs\", \"sgd\", \"adam\"]}\n",
    "    pipeline = make_pipeline(normalisers, mlp)\n",
    "    g_search = GridSearchCV(pipeline, param_grid=pipeline_param, cv=21, n_jobs=-1)\n",
    "\n",
    "    return (g_search.score(X_test, y_test), g_search.best_score_, g_search.best_params_), g_search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
